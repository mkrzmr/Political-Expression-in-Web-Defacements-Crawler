#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jul 26 14:10:38 2018

@author: Michael Kurzmeier

WARNING : This must be run with NoScript enabled. 
"""
# This script crawls the Zone-H special archive. It attempts a given number of random page. It writes a set of metadata to 
# a file and saves each page as a screenshot and html file. It expects the following files
# to be present in the working directory:
#     output_list_random.csv (use the supplied file)
#     error_log_random.csv (can be empty, will be filled as errors occur)
    
# It also needs:
#     A Firefox installation for selenium to use for browsing - see the  extension path below (line 46)
#     NoScript extension for Firefox - see extension Id below (line 49)

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import UnexpectedAlertPresentException
from random import randint
import urllib.request
 

#time.sleep(5)
#browser.quit()

#higest number Dec 11th 2018 32040219

print("Randomly selecting archive pages, highest known id 32040219")
top = 32040219 #check the highest number and adjust, otherwise this can also be used to define the crawl range.
n= randint(10000,top) 
range = input("Please enter range: ") #number of attempts
pcount = 0
data = []

output = open('output_list_random.csv', 'a')
error = open ('error_log_random.csv', 'a')

extension_dir = "/home/michael/.mozilla/firefox/r2356hsr.default/extensions/" #make relative
browser = webdriver.Firefox()
extensions = [
    '{73a6fe31-595d-460b-a920-fcc0f8843232}.xpi', #noscript
    ]
 
for extension in extensions:
    browser.install_addon(extension_dir + extension, temporary=True)


def process_page(n): #main loop
    url = ("http://zone-h.com/mirror/id/"+str(n))
    
    try:
        browser.get(url)
        browser.maximize_window()
    except UnexpectedAlertPresentException: #handles popups
        pass
        
    # Wait 10 seconds for page to load //*[@id="propdeface"]/iframe
    timeout = 5 #time limit for waiting
    try:
        # Wait until the frame is loaded.
        WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, "//iframe")))        
    except TimeoutException:
        print("Timed out waiting for page to load")
        e = (str(n)+', timeout \n')
        error.write(e) #writes error log
    except UnexpectedAlertPresentException: #handles popups
        pass
    
    try:
        browser.find_element_by_xpath ("//*[@id='error']/p/img") # gives better error logs
        print ("Invaild")
        e = (str(n)+ ', Invalid \n')
    except NoSuchElementException:
        pass

    
    try: #Selenium attempts to find all metadata the is on the page
        date = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[1]")  
        notifyer = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[2]/ul/li[1]")
        domain = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[2]/ul/li[2]")
        ip = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[2]/ul/li[3]")
        system = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[3]/ul/li[1]")
        server = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[3]/ul/li[2]")
        data = [date, notifyer, domain, ip, system, server]
    
    #titles_element = browser.find_elements_by_xpath("//*[@id='propdeface']/ul") #this should find elements individually and write to a list

        data_text = [x.text for x in data]
        try: #ip and country flag not present in all mirrors
            country = browser.find_element_by_xpath("//*[@id='propdeface']/ul/li[2]/ul/li[3]/img")  
            img = country.get_attribute("alt")
            data_text.append (img) #alt text from counrty flag
        except NoSuchElementException:
            img = "no ip"
            data_text.append(img)
        
        data_text.append (str(n))
        data_text.append (url)
        print (data_text) #testing
        x = (str(','.join(data_text)+'\n'))
        output.write(x)
        print(x)
    except NoSuchElementException:
        print("page did not load properly")
        e = (str(n)+', page did not load \n')
        error.write(e) #writes error log
    except UnexpectedAlertPresentException: #handles popups
        pass
        
    
    try: 
        iframe_url = browser.find_element_by_xpath("//*[@id='propdeface']/iframe") #open iframe and takes full page screenshot # needs to wait
        iframe = iframe_url.get_attribute("src")
        browser.get(iframe)
        browser.save_screenshot('screenshot_'+str(n)+'.png') #takes a screenshot
        urllib.request.urlretrieve(iframe, str(n)+'.txt') #saves the page as .txt
    except NoSuchElementException:
        print("iframe did not load")
        e = (str(n)+', iframe did not load \n')
        error.write(e) #writes error log
    except UnexpectedAlertPresentException: #handles popups
        pass

    data =[]
    

while (pcount < int(range)):
    process_page(n)
    n = randint(10000,top)
    pcount += 1
    
browser.quit()
output.close()
error.close()